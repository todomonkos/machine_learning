{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Importing libraries</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBJeCxIaq7TK"
   },
   "outputs": [],
   "source": [
    "%pip install nltk transformers wordcloud matplotlib pandas dataclasses tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from transformers import pipeline, AutoModelForSequenceClassification, DistilBertTokenizer\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import logging\n",
    "import json\n",
    "import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Document Analyzer</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nsPU8N11yOyO"
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "file_path = Path(\"file-path\")\n",
    "SUMMARIZER_MODEL = \"facebook/bart-large-cnn\"\n",
    "CHUNKER_MODEL = \"BlueOrangeDigital/distilbert-cross-segment-document-chunking\"\n",
    "BART_MAX_LENGTH = 512  # Maximum input length for BART model\n",
    "BART_CONFIG = {\n",
    "    \"max_length\": 70,\n",
    "    \"min_length\": 30,\n",
    "    \"length_penalty\": 2.0,\n",
    "    \"num_beams\": 4,\n",
    "    \"early_stopping\": True,\n",
    "    \"no_repeat_ngram_size\": 3,\n",
    "}\n",
    "\n",
    "\n",
    "class DocumentAnalyzer:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the DocumentAnalyzer with necessary resources and with error handling.\n",
    "\n",
    "        This method sets up:\n",
    "         - NLTK resources ('punkt' for tokenization, 'stopwords', 'averaged_perceptron_tagger' for tagging grammatical role)\n",
    "         - Summarization pipeline using BART\n",
    "\n",
    "        Raises:\n",
    "         - LookupError, if NLTK resources cannot be downloaded\n",
    "         - ImportError, if required libraries are not installed\n",
    "         - Exception, for other initialization errors\n",
    "        \"\"\"\n",
    "        self.logger = logging.getLogger(f\"{__name__}.DocumentAnalyzer\")\n",
    "\n",
    "        try:\n",
    "            self.logger.info(\"Initializing DocumentAnalyzer...\")\n",
    "\n",
    "            # Initialize DistilBERT model\n",
    "            self.logger.info(\"Setting up DistilBERT model for document segmentation...\")\n",
    "            self.chunker_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                CHUNKER_MODEL,\n",
    "                num_labels=2,\n",
    "                id2label={0: \"SAME\", 1: \"DIFFERENT\"},\n",
    "                label2id={\"SAME\": 0, \"DIFFERENT\": 1},\n",
    "            )\n",
    "            self.chunker_tokenizer = DistilBertTokenizer.from_pretrained(CHUNKER_MODEL)\n",
    "            self.chunking_pipeline = pipeline(\n",
    "                \"text-classification\",\n",
    "                model=self.chunker_model,\n",
    "                tokenizer=self.chunker_tokenizer,\n",
    "                top_k=None,\n",
    "            )\n",
    "\n",
    "            # Download NLTK resources\n",
    "            self.logger.info(\"Downloading NLTK resources...\")\n",
    "            for resource in [\"punkt\", \"stopwords\", \"averaged_perceptron_tagger\"]:\n",
    "                try:\n",
    "                    nltk.download(resource, quiet=True)\n",
    "                except Exception as e:\n",
    "                    raise LookupError(\n",
    "                        f\"Failed to download NLTK resource '{resource}': {str(e)}\"\n",
    "                    )\n",
    "\n",
    "            # Initialize stopwords\n",
    "            self.logger.info(\"Setting up stopwords...\")\n",
    "            self.stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "            # Initialize BART summarizer with optimal config\n",
    "            self.logger.debug(\"Setting up BART summarization pipeline...\")\n",
    "            self.summarizer = pipeline(\n",
    "                task=\"summarization\",\n",
    "                model=SUMMARIZER_MODEL,\n",
    "                framework=\"pt\",\n",
    "                device=-1,\n",
    "                **BART_CONFIG,\n",
    "            )\n",
    "            self.logger.debug(\"Summarizer pipeline initialized successfully\")\n",
    "\n",
    "            self.logger.info(\"DocumentAnalyzer initialization completed successfully!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during DocumentAnalyzer initialization: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def load_and_sample_data(self, file_path, sample_size=500):\n",
    "        \"\"\"\n",
    "        Load CSV file and create a balanced sample across all categories\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Path to the CSV file\n",
    "            sample_size (int): Total desired sample size (with a default of 500)\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Balanced sample of articles\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Read the CSV file\n",
    "            df = pd.read_csv(\n",
    "                file_path,\n",
    "                sep=\"\\t\",  # Tab as delimiter\n",
    "                engine=\"python\",  # More flexible parsing\n",
    "                on_bad_lines=\"warn\",  # Don't fail on problematic lines\n",
    "                encoding=\"utf-8\",  # Explicitly set encoding\n",
    "            )\n",
    "\n",
    "            # Ensure required columns exist\n",
    "            required_columns = [\"category\", \"content\"]\n",
    "            if not all(col in df.columns for col in required_columns):\n",
    "                raise ValueError(f\"CSV must contain these columns: {required_columns}\")\n",
    "\n",
    "            # Clean the data by removing rows with missing values in the 'category' and 'content' columns\n",
    "            df = df.dropna(subset=[\"category\", \"content\"])\n",
    "\n",
    "            # Get unique categories\n",
    "            categories = df[\"category\"].unique()\n",
    "            samples_per_category = sample_size // len(categories)\n",
    "\n",
    "            # Create balanced sample\n",
    "            sampled_df = pd.DataFrame()\n",
    "            for category in categories:\n",
    "                category_df = df[df[\"category\"] == category]\n",
    "                category_sample = category_df.sample(\n",
    "                    n=min(samples_per_category, len(category_df)),\n",
    "                    random_state=42,  # For reproducibility\n",
    "                )\n",
    "                sampled_df = pd.concat([sampled_df, category_sample])\n",
    "\n",
    "            # In case we need more samples to reach the target size\n",
    "            remaining_samples = sample_size - len(sampled_df)\n",
    "            if remaining_samples > 0:\n",
    "                # Sample randomly from all categories to make up the difference\n",
    "                remaining_df = df[~df.index.isin(sampled_df.index)]\n",
    "                additional_samples = remaining_df.sample(\n",
    "                    n=min(remaining_samples, len(remaining_df)), random_state=42\n",
    "                )\n",
    "                sampled_df = pd.concat([sampled_df, additional_samples])\n",
    "\n",
    "            # Shuffle the final dataset\n",
    "            sampled_df = sampled_df.sample(frac=1, random_state=42).reset_index(\n",
    "                drop=True\n",
    "            )\n",
    "\n",
    "            print(f\"\\nCreated balanced sample with {len(sampled_df)} articles\")\n",
    "            print(\"\\nCategory distribution:\")\n",
    "            print(sampled_df[\"category\"].value_counts())\n",
    "\n",
    "            return sampled_df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading and sampling data: {str(e)}\")\n",
    "            print(\"\\nDataFrame info before error:\")\n",
    "            try:\n",
    "                print(df.info())\n",
    "            except Exception as e:\n",
    "                print(\"Could not print DataFrame info\")\n",
    "            return None\n",
    "\n",
    "    def process_dataframe(self, df, text_column):\n",
    "        \"\"\"Process text data from a pandas DataFrame column\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): Input DataFrame\n",
    "            text_column (str): Name of the column containing text data\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame with added analysis columns\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create a copy of the dataframe for processing\n",
    "            processed_df = df.copy()\n",
    "\n",
    "            # Create new columns for analysis results\n",
    "            processed_df[\"num_sentences\"] = processed_df[text_column].apply(\n",
    "                lambda x: len(self.basic_stats(x)[\"sentences\"])\n",
    "            )\n",
    "            processed_df[\"num_words\"] = processed_df[text_column].apply(\n",
    "                lambda x: self.basic_stats(x)[\"num_words\"]\n",
    "            )\n",
    "            processed_df[\"avg_word_length\"] = processed_df[text_column].apply(\n",
    "                lambda x: self.basic_stats(x)[\"avg_word_length\"]\n",
    "            )\n",
    "            processed_df[\"avg_sentence_length\"] = processed_df[text_column].apply(\n",
    "                lambda x: self.basic_stats(x)[\"avg_sentence_length\"]\n",
    "            )\n",
    "            processed_df[\"common_words\"] = processed_df[text_column].apply(\n",
    "                lambda x: self.get_common_words(x)\n",
    "            )\n",
    "            processed_df[\"common_phrases\"] = processed_df[text_column].apply(\n",
    "                lambda x: self.get_common_phrases(x)\n",
    "            )\n",
    "\n",
    "            return processed_df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing DataFrame: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def basic_stats(self, text):\n",
    "        \"\"\"Calculate basic text statistics\"\"\"\n",
    "        try:\n",
    "            # Split into sentences\n",
    "            sentences = sent_tokenize(text)\n",
    "\n",
    "            # Split into words and clean\n",
    "            words = word_tokenize(text.lower())\n",
    "            words = [word for word in words if word.isalnum()]  # Remove punctuation\n",
    "\n",
    "            # Calculate statistics\n",
    "            num_sentences = len(sentences)\n",
    "            num_words = len(words)\n",
    "            avg_word_length = (\n",
    "                sum(len(word) for word in words) / num_words if num_words > 0 else 0\n",
    "            )\n",
    "            avg_sentence_length = num_words / num_sentences if num_sentences > 0 else 0\n",
    "\n",
    "            return {\n",
    "                \"sentences\": sentences,\n",
    "                \"num_sentences\": num_sentences,\n",
    "                \"num_words\": num_words,\n",
    "                \"avg_word_length\": round(avg_word_length, 2),\n",
    "                \"avg_sentence_length\": round(avg_sentence_length, 2),\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating basic stats: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def get_common_words(self, text, n=10):\n",
    "        \"\"\"Find the n most common words in the text\"\"\"\n",
    "        try:\n",
    "            # Tokenize and clean text\n",
    "            words = word_tokenize(text.lower())\n",
    "            words = [\n",
    "                word for word in words if word.isalnum() and word not in self.stop_words\n",
    "            ]\n",
    "\n",
    "            # Count frequencies\n",
    "            word_freq = Counter(words)\n",
    "\n",
    "            # Get top n words\n",
    "            common_words = word_freq.most_common(n)\n",
    "            return [(word, freq) for word, freq in common_words]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error finding common words: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def get_common_phrases(self, text, n=5, phrase_length=2):\n",
    "        \"\"\"Find the n most common phrases of specified length\"\"\"\n",
    "        try:\n",
    "            # Tokenize and clean text\n",
    "            words = word_tokenize(text.lower())\n",
    "            words = [\n",
    "                word for word in words if word.isalnum() and word not in self.stop_words\n",
    "            ]\n",
    "\n",
    "            # Generate n-grams\n",
    "            phrases = list(ngrams(words, phrase_length))\n",
    "\n",
    "            # Count frequencies\n",
    "            phrase_freq = Counter(phrases)\n",
    "\n",
    "            # Get top n phrases\n",
    "            common_phrases = phrase_freq.most_common(n)\n",
    "            return [(\" \".join(phrase), freq) for phrase, freq in common_phrases]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error finding common phrases: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def create_wordcloud(self, text, save_path=None):\n",
    "        \"\"\"Generate and optionally save a word cloud visualization\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to create word cloud from\n",
    "            save_path (str or Path, optional): If provided, save the wordcloud to this path\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create WordCloud object\n",
    "            wordcloud = WordCloud(\n",
    "                width=800,\n",
    "                height=400,\n",
    "                background_color=\"white\",\n",
    "                stopwords=self.stop_words,\n",
    "                min_font_size=10,\n",
    "            ).generate(text)\n",
    "\n",
    "            # Create the plot\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            # Save or display\n",
    "            if save_path:\n",
    "                plt.savefig(save_path)  # Find the figures in the 'results' dictionary\n",
    "                plt.close()  # Close the figure to free memory\n",
    "            else:\n",
    "                plt.show()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating word cloud: {str(e)}\")\n",
    "\n",
    "    def generate_summary(\n",
    "        self,\n",
    "        text,\n",
    "        max_length=BART_CONFIG[\"max_length\"],\n",
    "        min_length=BART_CONFIG[\"min_length\"],\n",
    "        output_path=None,\n",
    "        document_id=None,\n",
    "        metadata=None,\n",
    "    ):\n",
    "        \"\"\"Generate a summary using an adaptive single/two-pass approach based on text length.\"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Generating summary for document {document_id}\")\n",
    "\n",
    "            # Count input tokens\n",
    "            input_tokens = len(self.summarizer.tokenizer(text)[\"input_ids\"])\n",
    "            self.logger.debug(f\"Input tokens: {input_tokens}\")\n",
    "\n",
    "            # Initialize metadata\n",
    "            summary_metadata = {\n",
    "                \"document_id\": document_id,\n",
    "                \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "                \"input_tokens\": input_tokens,\n",
    "                \"single_pass\": True,\n",
    "            }\n",
    "\n",
    "            # For long texts, use chunking approach\n",
    "            if input_tokens > BART_MAX_LENGTH:\n",
    "                self.logger.info(\n",
    "                    f\"Long text detected ({input_tokens} tokens). Using chunking approach.\"\n",
    "                )\n",
    "                chunks, chunk_stats = self._split_into_chunks(text)\n",
    "                chunk_summaries = []\n",
    "\n",
    "                for i, chunk in enumerate(chunks, 1):\n",
    "                    self.logger.debug(f\"Processing chunk {i}/{len(chunks)}\")\n",
    "                    chunk_summary = self.summarizer(\n",
    "                        chunk,\n",
    "                        max_length=max_length,\n",
    "                        min_length=min_length,\n",
    "                        truncation=True,\n",
    "                    )[0][\"summary_text\"]\n",
    "                    chunk_summaries.append(chunk_summary)\n",
    "\n",
    "                # Combine chunk summaries and generate final summary\n",
    "                self.logger.debug(\"Generating final summary from chunks\")\n",
    "                intermediate_summary = \" \".join(chunk_summaries)\n",
    "                final_summary = self.summarizer(\n",
    "                    intermediate_summary,\n",
    "                    max_length=max_length,\n",
    "                    min_length=min_length,\n",
    "                    truncation=True,\n",
    "                )[0][\"summary_text\"]\n",
    "\n",
    "                summary_metadata.update(\n",
    "                    {\n",
    "                        \"single_pass\": False,\n",
    "                        \"num_chunks\": len(chunks),\n",
    "                        \"chunk_stats\": chunk_stats,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                summary_text = final_summary\n",
    "            else:\n",
    "                self.logger.debug(\"Using single-pass approach\")\n",
    "                summary_text = self.summarizer(\n",
    "                    text, max_length=max_length, min_length=min_length, truncation=True\n",
    "                )[0][\"summary_text\"]\n",
    "\n",
    "            # Count output tokens\n",
    "            output_tokens = len(self.summarizer.tokenizer(summary_text)[\"input_ids\"])\n",
    "            summary_metadata[\"output_tokens\"] = output_tokens\n",
    "\n",
    "            # Add source CSV metadata if provided\n",
    "            if metadata:\n",
    "                summary_metadata[\"source\"] = {\n",
    "                    \"category\": metadata.get(\"category\"),\n",
    "                    \"filename\": metadata.get(\"filename\"),\n",
    "                    \"title\": metadata.get(\"title\"),\n",
    "                    \"index\": metadata.get(\"index\"),\n",
    "                }\n",
    "\n",
    "            summary_result = {\"summary\": summary_text, \"metadata\": summary_metadata}\n",
    "\n",
    "            self.logger.info(\n",
    "                f\"Successfully generated summary for document {document_id}\"\n",
    "            )\n",
    "            return summary_result\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(\n",
    "                f\"Error in generate_summary for document {document_id}: {str(e)}\"\n",
    "            )\n",
    "            return {\"error\": str(e), \"summary\": \"\"}\n",
    "\n",
    "    def _split_into_chunks(self, text, max_chunk_size=1024):\n",
    "        \"\"\"Split text into chunks using DistilBERT-based segmentation with token-aware sizing\"\"\"\n",
    "        try:\n",
    "            # Initialize statistics\n",
    "            stats = {\n",
    "                \"total_words\": len(text.split()),\n",
    "                \"breakpoints\": [],\n",
    "                \"breakpoint_scores\": [],\n",
    "                \"chunk_sizes\": [],\n",
    "                \"chunk_sentence_counts\": [],\n",
    "                \"processing_start_time\": datetime.datetime.now().isoformat(),\n",
    "            }\n",
    "\n",
    "            # Input validation\n",
    "            if not text or not isinstance(text, str):\n",
    "                self.logger.warning(\"Empty or invalid input text provided\")\n",
    "                return [], {\"error\": \"Empty or invalid input\"}\n",
    "\n",
    "            # Split into sentences\n",
    "            sentences = sent_tokenize(text)\n",
    "            if not sentences:\n",
    "                self.logger.warning(\"No sentences found in input text\")\n",
    "                return [text[:max_chunk_size]], {\n",
    "                    \"error\": \"No sentences found\",\n",
    "                    \"stats\": stats,\n",
    "                }\n",
    "\n",
    "            stats[\"total_sentences\"] = len(sentences)\n",
    "            self.logger.debug(f\"Processing {len(sentences)} sentences\")\n",
    "\n",
    "            # Create sentence pairs for classification\n",
    "            sent_pairs = [\n",
    "                sentences[i] + \" [SEP] \" + sentences[i + 1]\n",
    "                for i in range(len(sentences) - 1)\n",
    "            ]\n",
    "\n",
    "            if not sent_pairs:\n",
    "                return [text[:max_chunk_size]], {\n",
    "                    \"error\": \"No sentence pairs for classification\"\n",
    "                }\n",
    "\n",
    "            # Process sentence pairs with chunking pipeline\n",
    "            try:\n",
    "                attributions = []\n",
    "                for pair in tqdm(\n",
    "                    sent_pairs,\n",
    "                    desc=\"Classifying sentence pairs\",\n",
    "                    disable=len(sent_pairs) < 10,\n",
    "                ):\n",
    "                    # Get raw pipeline output\n",
    "                    result = self.chunking_pipeline(pair)\n",
    "                    self.logger.debug(f\"Pipeline output: {result}\")\n",
    "\n",
    "                    # Handle the pipeline output based on its structure\n",
    "                    if (\n",
    "                        isinstance(result, list)\n",
    "                        and len(result) > 0\n",
    "                        and isinstance(result[0], list)\n",
    "                    ):\n",
    "                        # Get the inner list of predictions\n",
    "                        predictions = result[0]\n",
    "                        # Convert pipeline output to score dictionary\n",
    "                        scores = {}\n",
    "                        for pred in predictions:\n",
    "                            if (\n",
    "                                isinstance(pred, dict)\n",
    "                                and \"label\" in pred\n",
    "                                and \"score\" in pred\n",
    "                            ):\n",
    "                                scores[pred[\"label\"]] = pred[\"score\"]\n",
    "\n",
    "                        # Ensure we have both scores\n",
    "                        diff_score = scores.get(\"DIFFERENT\", 0.0)\n",
    "                        same_score = scores.get(\n",
    "                            \"SAME\", 1.0 - diff_score\n",
    "                        )  # If one is missing, assume complementary\n",
    "\n",
    "                        attributions.append(\n",
    "                            {\"DIFFERENT\": diff_score, \"SAME\": same_score}\n",
    "                        )\n",
    "                    else:\n",
    "                        self.logger.warning(\n",
    "                            f\"Unexpected pipeline output format: {type(result)}\"\n",
    "                        )\n",
    "                        # Add neutral scores if we can't interpret the output\n",
    "                        attributions.append({\"DIFFERENT\": 0.5, \"SAME\": 0.5})\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Pipeline processing failed: {str(e)}\")\n",
    "                return [text[:max_chunk_size]], {\n",
    "                    \"error\": f\"Pipeline processing failed: {str(e)}\"\n",
    "                }\n",
    "\n",
    "            # Find breakpoints with token-aware sizing\n",
    "            breakpoints = []\n",
    "            current_chunk_tokens = 0\n",
    "            min_chunk_tokens = int(BART_MAX_LENGTH * 0.2)  # Minimum 20% of max length\n",
    "\n",
    "            # Pre-calculate token counts for all sentences\n",
    "            sentence_tokens = [\n",
    "                len(self.summarizer.tokenizer(sent)[\"input_ids\"]) for sent in sentences\n",
    "            ]\n",
    "\n",
    "            # Find semantic breakpoints\n",
    "            for i, scores in enumerate(attributions):\n",
    "                try:\n",
    "                    # Get the DIFFERENT score directly from the attribution\n",
    "                    diff_score = scores.get(\"DIFFERENT\", 0.0)\n",
    "\n",
    "                    # Add tokens for current sentence\n",
    "                    current_chunk_tokens += sentence_tokens[i]\n",
    "\n",
    "                    self.logger.debug(\n",
    "                        f\"Position {i + 1}: tokens={current_chunk_tokens}, \"\n",
    "                        f\"min_required={min_chunk_tokens}, diff_score={diff_score:.3f}\"\n",
    "                    )\n",
    "\n",
    "                    # Create break if we have enough tokens and strong semantic difference\n",
    "                    if current_chunk_tokens >= min_chunk_tokens and diff_score > 0.6:\n",
    "                        breakpoints.append(i + 1)\n",
    "                        stats[\"breakpoint_scores\"].append(\n",
    "                            {\n",
    "                                \"position\": i + 1,\n",
    "                                \"diff_score\": diff_score,\n",
    "                                \"chunk_tokens\": current_chunk_tokens,\n",
    "                            }\n",
    "                        )\n",
    "                        self.logger.debug(\n",
    "                            f\"Added breakpoint at {i + 1}: tokens={current_chunk_tokens}, diff_score={diff_score:.3f}\"\n",
    "                        )\n",
    "                        # Reset token count for next chunk\n",
    "                        current_chunk_tokens = 0\n",
    "                        # Start counting from next sentence\n",
    "                        continue\n",
    "\n",
    "                except Exception as e:\n",
    "                    self.logger.debug(f\"Skipping classification at index {i}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "            # If no breaks found and text is too long, force a break at best semantic point\n",
    "            if not breakpoints and current_chunk_tokens > max_chunk_size:\n",
    "                # Find best breakpoint based on semantic scores\n",
    "                best_break = max(\n",
    "                    range(len(attributions)), key=lambda i: attributions[i][\"DIFFERENT\"]\n",
    "                )\n",
    "                if best_break > 0:\n",
    "                    breakpoints.append(best_break + 1)\n",
    "                    stats[\"breakpoint_scores\"].append(\n",
    "                        {\n",
    "                            \"position\": best_break + 1,\n",
    "                            \"diff_score\": attributions[best_break][\"DIFFERENT\"],\n",
    "                            \"chunk_tokens\": sum(sentence_tokens[: best_break + 1]),\n",
    "                        }\n",
    "                    )\n",
    "                    self.logger.debug(f\"Forced break at {best_break + 1} due to length\")\n",
    "\n",
    "            # Create chunks based on breakpoints\n",
    "            chunks = []\n",
    "            current_chunk = []\n",
    "            current_size = 0\n",
    "            current_sent_count = 0\n",
    "\n",
    "            # Add debug logging for sentence lengths\n",
    "            self.logger.debug(\"Sentence token counts:\")\n",
    "            for i, tokens in enumerate(sentence_tokens):\n",
    "                self.logger.debug(f\"Sentence {i + 1}: {tokens} tokens\")\n",
    "\n",
    "            for i, sentence in enumerate(sentences):\n",
    "                sentence_tokens_count = sentence_tokens[i]\n",
    "\n",
    "                # Create new chunk if we hit a breakpoint\n",
    "                if i in breakpoints:  # Changed from i+1 to i\n",
    "                    if current_chunk:  # Save current chunk if it exists\n",
    "                        chunk_text = \" \".join(current_chunk)\n",
    "                        chunks.append(chunk_text)\n",
    "                        stats[\"chunk_sizes\"].append(current_size)\n",
    "                        stats[\"chunk_sentence_counts\"].append(current_sent_count)\n",
    "                        self.logger.debug(\n",
    "                            f\"Created chunk with {current_size} tokens and {current_sent_count} sentences\"\n",
    "                        )\n",
    "\n",
    "                    # Start new chunk with current sentence\n",
    "                    current_chunk = [sentence]\n",
    "                    current_size = sentence_tokens_count\n",
    "                    current_sent_count = 1\n",
    "                else:\n",
    "                    current_chunk.append(sentence)\n",
    "                    current_size += sentence_tokens_count\n",
    "                    current_sent_count += 1\n",
    "\n",
    "            # Add the last chunk if it exists\n",
    "            if current_chunk:\n",
    "                chunk_text = \" \".join(current_chunk)\n",
    "                chunks.append(chunk_text)\n",
    "                stats[\"chunk_sizes\"].append(current_size)\n",
    "                stats[\"chunk_sentence_counts\"].append(current_sent_count)\n",
    "                self.logger.debug(\n",
    "                    f\"Created final chunk with {current_size} tokens and {current_sent_count} sentences\"\n",
    "                )\n",
    "\n",
    "            # Log breakpoint information for debugging\n",
    "            self.logger.debug(f\"Found breakpoints at positions: {breakpoints}\")\n",
    "            for score in stats[\"breakpoint_scores\"]:\n",
    "                self.logger.debug(\n",
    "                    f\"Breakpoint at {score['position']}: diff_score={score['diff_score']}, tokens={score['chunk_tokens']}\"\n",
    "                )\n",
    "\n",
    "            # Calculate final statistics\n",
    "            if chunks:\n",
    "                stats.update(\n",
    "                    {\n",
    "                        \"num_chunks\": len(chunks),\n",
    "                        \"avg_chunk_size\": sum(stats[\"chunk_sizes\"]) / len(chunks),\n",
    "                        \"max_chunk_size\": max(stats[\"chunk_sizes\"]),\n",
    "                        \"min_chunk_size\": min(stats[\"chunk_sizes\"]),\n",
    "                        \"avg_sentences_per_chunk\": sum(stats[\"chunk_sentence_counts\"])\n",
    "                        / len(chunks),\n",
    "                        \"processing_end_time\": datetime.datetime.now().isoformat(),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                self.logger.info(\"Chunk Statistics:\")\n",
    "                self.logger.info(f\"Total chunks: {stats['num_chunks']}\")\n",
    "                self.logger.info(\n",
    "                    f\"Average chunk size (tokens): {stats['avg_chunk_size']:.2f}\"\n",
    "                )\n",
    "                self.logger.info(\n",
    "                    f\"Max/Min chunk size: {stats['max_chunk_size']}/{stats['min_chunk_size']}\"\n",
    "                )\n",
    "                self.logger.info(f\"Breakpoints found: {len(breakpoints)}\")\n",
    "                self.logger.info(f\"Breakpoint positions: {breakpoints}\")\n",
    "                self.logger.info(\n",
    "                    f\"Avg sentences per chunk: {stats['avg_sentences_per_chunk']:.2f}\"\n",
    "                )\n",
    "\n",
    "                self.logger.debug(f\"Created {len(chunks)} chunks\")\n",
    "                return chunks, stats\n",
    "\n",
    "            return [text[:max_chunk_size]], {\n",
    "                \"error\": \"No chunks created\",\n",
    "                \"stats\": stats,\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in _split_into_chunks: {str(e)}\")\n",
    "            return [text[:max_chunk_size]], {\"error\": str(e)}\n",
    "\n",
    "    def save_summaries_to_json(self, summaries, output_path):\n",
    "        \"\"\"\n",
    "        Save summaries and their metadata to a JSON file\n",
    "\n",
    "        Args:\n",
    "            summaries (dict): Dictionary containing summaries and their metadata\n",
    "            output_path (str or Path): Path to save the JSON file\n",
    "\n",
    "        Returns:\n",
    "            dict: Status of the save operation\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Convert output_path to Path object\n",
    "            output_path = Path(output_path)\n",
    "\n",
    "            # Ensure the output directory exists\n",
    "            output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # Initialize the full structure if it doesn't exist\n",
    "            if not isinstance(summaries, dict) or \"summaries\" not in summaries:\n",
    "                summaries = {\n",
    "                    \"metadata\": {\n",
    "                        \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "                        \"version\": {\n",
    "                            \"bart_model\": SUMMARIZER_MODEL,\n",
    "                            \"chunker_model\": CHUNKER_MODEL,\n",
    "                            \"max_length\": BART_MAX_LENGTH,\n",
    "                            \"config\": BART_CONFIG,\n",
    "                        },\n",
    "                    },\n",
    "                    \"summaries\": [],\n",
    "                }\n",
    "\n",
    "            # Ensure summaries is a list\n",
    "            if \"summaries\" in summaries and not isinstance(\n",
    "                summaries[\"summaries\"], list\n",
    "            ):\n",
    "                summaries[\"summaries\"] = [summaries[\"summaries\"]]\n",
    "\n",
    "            # Write to JSON file with error handling\n",
    "            try:\n",
    "                with output_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(summaries, f, indent=4, ensure_ascii=False)\n",
    "            except Exception as e:\n",
    "                raise IOError(f\"Failed to write JSON file: {str(e)}\")\n",
    "\n",
    "            # Verify file was written\n",
    "            if not output_path.exists():\n",
    "                raise IOError(\"File was not created successfully\")\n",
    "\n",
    "            file_size = output_path.stat().st_size\n",
    "            self.logger.info(\n",
    "                f\"Summaries successfully saved to {output_path} ({file_size} bytes)\"\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"file_path\": str(output_path),\n",
    "                \"file_size_bytes\": file_size,\n",
    "                \"timestamp\": summaries[\"metadata\"][\"timestamp\"],\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error saving summaries to JSON: {str(e)}\"\n",
    "            self.logger.error(error_msg)\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error\": error_msg,\n",
    "                \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "            }\n",
    "\n",
    "    def save_analysis(self, filepath, analysis_results):\n",
    "        \"\"\"Save analysis results to a CSV file\"\"\"\n",
    "        try:\n",
    "            # Convert complex columns to string representation\n",
    "            analysis_results[\"common_words\"] = analysis_results[\"common_words\"].apply(\n",
    "                str\n",
    "            )\n",
    "            analysis_results[\"common_phrases\"] = analysis_results[\n",
    "                \"common_phrases\"\n",
    "            ].apply(str)\n",
    "\n",
    "            # Save to CSV\n",
    "            analysis_results.to_csv(filepath, index=False)\n",
    "            self.logger.info(f\"Analysis results saved to {filepath}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error saving analysis results: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Basic functionality test</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests():\n",
    "    \"\"\"Your existing run_tests code\"\"\"\n",
    "    analyzer = DocumentAnalyzer()\n",
    "    test_df = pd.DataFrame({\n",
    "        'content': ['This is a test article. It has two sentences.',\n",
    "                   'Another test article. With multiple sentences. Testing.'],\n",
    "        'category': ['test', 'test']\n",
    "    })\n",
    "    results = analyzer.process_dataframe(test_df, 'content')\n",
    "    assert not results.empty, \"Processing should return non-empty DataFrame\"\n",
    "    assert 'num_sentences' in results.columns, \"Results should include sentence count\"\n",
    "    assert 'num_words' in results.columns, \"Results should include word count\"\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Analysis interface</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class AnalysisParameters:\n",
    "    \"\"\"Class for holding document analysis parameters\"\"\"\n",
    "\n",
    "    input_file: str\n",
    "    sample_size: int = 500\n",
    "    text_column: str = \"content\"\n",
    "    wordclouds: int = 5\n",
    "    save_wordclouds: bool = False\n",
    "    output_file: str = \"analysis_results.csv\"\n",
    "    output_dir: str = \"results\"\n",
    "    run_tests: bool = False\n",
    "    debug: bool = False\n",
    "    summarize: bool = False\n",
    "    max_summary_length: int = 70\n",
    "    min_summary_length: int = 30\n",
    "\n",
    "\n",
    "class DocumentAnalysisCLI:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize CLI with configured logging.\"\"\"\n",
    "        self.logger = self._setup_logging()\n",
    "        self.analyzer = None\n",
    "\n",
    "    def _setup_logging(self):\n",
    "        \"\"\"Configure logging with formatted output.\"\"\"\n",
    "        # Create formatter\n",
    "        formatter = logging.Formatter(\n",
    "            \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "            datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "        )\n",
    "\n",
    "        # Create console handler\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setFormatter(formatter)\n",
    "\n",
    "        # Get logger\n",
    "        logger = logging.getLogger(f\"{__name__}.DocumentAnalysisCLI\")\n",
    "        logger.addHandler(console_handler)\n",
    "\n",
    "        return logger\n",
    "\n",
    "    def _configure_logging_level(self, debug: bool):\n",
    "        \"\"\"Configure logging level based on debug parameter.\"\"\"\n",
    "        root_logger = logging.getLogger()\n",
    "\n",
    "        # Remove existing handlers\n",
    "        for handler in root_logger.handlers[:]:\n",
    "            root_logger.removeHandler(handler)\n",
    "\n",
    "        # Create new handler with formatter\n",
    "        handler = logging.StreamHandler()\n",
    "        formatter = logging.Formatter(\n",
    "            \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "            datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "        )\n",
    "        handler.setFormatter(formatter)\n",
    "        root_logger.addHandler(handler)\n",
    "\n",
    "        # Set logging level\n",
    "        level = logging.DEBUG if debug else logging.INFO\n",
    "        root_logger.setLevel(level)\n",
    "        self.logger.setLevel(level)\n",
    "\n",
    "        self.logger.debug(\n",
    "            \"Logging level set to DEBUG\" if debug else \"Logging level set to INFO\"\n",
    "        )\n",
    "\n",
    "    def run(self, params: AnalysisParameters):\n",
    "        \"\"\"Main execution method.\"\"\"\n",
    "        try:\n",
    "            # Configure logging level\n",
    "            self._configure_logging_level(params.debug)\n",
    "\n",
    "            # Initialize the document analyzer\n",
    "            self.logger.info(\"Initializing Document Analyzer...\")\n",
    "            self.analyzer = DocumentAnalyzer()\n",
    "\n",
    "            # Add debug logging to verify methods\n",
    "            self.logger.debug(f\"Available analyzer methods: {dir(self.analyzer)}\")\n",
    "            self.logger.debug(f\"Analyzer class: {self.analyzer.__class__}\")\n",
    "\n",
    "            # Load and sample data\n",
    "            self.logger.info(f\"Loading data from {params.input_file}\")\n",
    "            sampled_df = self.analyzer.load_and_sample_data(\n",
    "                params.input_file, sample_size=params.sample_size\n",
    "            )\n",
    "\n",
    "            if sampled_df is None:\n",
    "                self.logger.error(\"Failed to load and sample data\")\n",
    "                return None\n",
    "\n",
    "            # Process the DataFrame\n",
    "            self.logger.info(\"Processing articles...\")\n",
    "            results = self.analyzer.process_dataframe(sampled_df, params.text_column)\n",
    "\n",
    "            if results is None:\n",
    "                self.logger.error(\"Failed to process articles\")\n",
    "                return None\n",
    "\n",
    "            # Generate word clouds if requested\n",
    "            if params.wordclouds > 0:\n",
    "                self._generate_wordclouds(params, sampled_df)\n",
    "\n",
    "            # Generate and save summaries if requested\n",
    "            if params.summarize:\n",
    "                self.logger.info(\"Starting summary generation...\")\n",
    "                summaries_data = []\n",
    "\n",
    "                # Configure progress bar with simpler formatting\n",
    "                with tqdm(\n",
    "                    total=len(results),\n",
    "                    desc=\"Generating summaries\",\n",
    "                    unit=\"doc\",\n",
    "                    leave=True,\n",
    "                ) as progress_bar:\n",
    "                    for idx, row in results.iterrows():\n",
    "                        try:\n",
    "                            # Create metadata dictionary from source CSV\n",
    "                            source_metadata = {\n",
    "                                \"category\": row[\"category\"],\n",
    "                                \"filename\": row[\"filename\"],\n",
    "                                \"title\": row[\"title\"],\n",
    "                                \"index\": idx,\n",
    "                            }\n",
    "\n",
    "                            summary_result = self.analyzer.generate_summary(\n",
    "                                text=row[params.text_column],\n",
    "                                max_length=params.max_summary_length,\n",
    "                                min_length=params.min_summary_length,\n",
    "                                document_id=str(idx),\n",
    "                                metadata=source_metadata,\n",
    "                            )\n",
    "\n",
    "                            summaries_data.append(summary_result)\n",
    "\n",
    "                            # Save summaries to JSON after each summary\n",
    "                            summaries_output_path = (\n",
    "                                Path(params.output_dir) / \"summaries.json\"\n",
    "                            )\n",
    "                            save_result = self.analyzer.save_summaries_to_json(\n",
    "                                {\n",
    "                                    \"metadata\": {\n",
    "                                        \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "                                        \"total_documents\": len(results),\n",
    "                                        \"successful_summaries\": len(summaries_data),\n",
    "                                        \"generation_date\": datetime.datetime.now().isoformat(),\n",
    "                                        \"parameters\": params.__dict__,\n",
    "                                    },\n",
    "                                    \"summaries\": summaries_data,\n",
    "                                },\n",
    "                                summaries_output_path,\n",
    "                            )\n",
    "\n",
    "                            if save_result[\"status\"] != \"success\":\n",
    "                                self.logger.error(\n",
    "                                    f\"Failed to save summary {idx}: {save_result.get('error', 'Unknown error')}\"\n",
    "                                )\n",
    "                                status = \"✗\"\n",
    "                            else:\n",
    "                                status = \"✓\" if \"error\" not in summary_result else \"✗\"\n",
    "\n",
    "                            # Update progress bar with basic status\n",
    "                            progress_bar.set_postfix(\n",
    "                                {\"status\": status, \"category\": row[\"category\"][:10]}\n",
    "                            )\n",
    "                            progress_bar.update(1)\n",
    "\n",
    "                        except Exception as e:\n",
    "                            self.logger.error(\n",
    "                                f\"Error processing document {idx}: {str(e)}\"\n",
    "                            )\n",
    "                            progress_bar.set_postfix({\"status\": \"error\"})\n",
    "                            continue\n",
    "\n",
    "                self.logger.info(\n",
    "                    f\"Summary generation completed. Processed {len(summaries_data)} documents.\"\n",
    "                )\n",
    "\n",
    "            # Save analysis results\n",
    "            output_path = Path(params.output_dir) / params.output_file\n",
    "            self.analyzer.save_analysis(output_path, results)\n",
    "            self.logger.info(f\"Analysis results saved to {output_path}\")\n",
    "\n",
    "            # Run tests if requested\n",
    "            if params.run_tests:\n",
    "                self.logger.info(\"Running tests...\")\n",
    "                run_tests()\n",
    "\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during execution: {str(e)}\")\n",
    "            if params.debug:\n",
    "                import traceback\n",
    "\n",
    "                self.logger.debug(traceback.format_exc())\n",
    "            return None\n",
    "\n",
    "    def _generate_wordclouds(self, params, sampled_df):\n",
    "        \"\"\"Helper method to generate word clouds.\"\"\"\n",
    "        self.logger.info(f\"Generating {params.wordclouds} word clouds...\")\n",
    "        random_articles = sampled_df.sample(\n",
    "            n=min(params.wordclouds, len(sampled_df)), random_state=42\n",
    "        )[params.text_column]\n",
    "\n",
    "        for i, article in enumerate(random_articles, 1):\n",
    "            self.logger.info(f\"Generating word cloud {i}/{params.wordclouds}\")\n",
    "            if params.save_wordclouds:\n",
    "                output_path = Path(params.output_dir) / f\"wordcloud_{i}.png\"\n",
    "                self.analyzer.create_wordcloud(article, save_path=output_path)\n",
    "            else:\n",
    "                self.analyzer.create_wordcloud(article)\n",
    "\n",
    "\n",
    "def run_analysis(input_file: str, output_path: str):\n",
    "    \"\"\"Run document analysis with specified parameters.\"\"\"\n",
    "    params = AnalysisParameters(\n",
    "        input_file=input_file,\n",
    "        sample_size=500,\n",
    "        text_column=\"content\",\n",
    "        wordclouds=5,\n",
    "        save_wordclouds=True,  # Changed to True to save wordclouds\n",
    "        output_file=\"analysis_results.csv\",\n",
    "        output_dir=\"results\",\n",
    "        run_tests=False,\n",
    "        debug=True,  # Changed to True for better debugging\n",
    "        summarize=True,  # Changed to True to generate summaries\n",
    "        max_summary_length=BART_CONFIG[\"max_length\"],\n",
    "        min_summary_length=BART_CONFIG[\"min_length\"],\n",
    "    )\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    Path(params.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Run the analysis\n",
    "    cli = DocumentAnalysisCLI()\n",
    "    return cli.run(params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Analysis</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before running this cell make sure to provide the correct file path above in the constants \n",
    "results = run_analysis(input_file=str(file_path), output_path=\"results\")\n",
    "\n",
    "if results is not None:\n",
    "    print(\"Analysis completed successfully!\")\n",
    "    \n",
    "    # Display some information about the results\n",
    "    print(f\"\\nAnalyzed {len(results)} articles\")\n",
    "    print(\"\\nColumns in results:\")\n",
    "    print(results.columns.tolist())\n",
    "    \n",
    "    # Show a sample of the summaries if they were generated\n",
    "    if 'summary' in results.columns:\n",
    "        print(\"\\nSample summary:\")\n",
    "        print(results['summary'].iloc[0])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
